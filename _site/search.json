[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Coding Code: Qualitative Methods for Investigating Data Science Skills",
    "section": "",
    "text": "Students are creative thinkers, and their code provides a window into their learning process. A qualitative analysis of a student’s code can provide insight into their creative process and collective learning process, rather than focusing solely on whether the code is executable. Moreover, qualitative methods allow for the comparison across students’ code to identify learning trajectories that may exist and potential growth points."
  },
  {
    "objectID": "blog.html#structure-of-tutorial",
    "href": "blog.html#structure-of-tutorial",
    "title": "A Guide to Qualitatively Analyzing Computing Code",
    "section": "Structure of Tutorial",
    "text": "Structure of Tutorial\nThe pages of this website correspond to different aspects of the analysis process as pictured above.\n\nI discuss how one selects a unit of analysis\nI outline how descriptive codes are created\nI walk through how codes can be collapsed into emergent themes\nI describe how student interviews allowed me to dig deeper into the themes that emerged\nI talk through how to make comparisons across individual’s themes"
  },
  {
    "objectID": "analysis-framework.html",
    "href": "analysis-framework.html",
    "title": "Step 1: Selecting a Unit of Analysis",
    "section": "",
    "text": "The process of data analysis begins by identifying segments in your data set that are responsive to your research questions” (Merriam and Tisdell 2016). These segments form the units of analysis (UOA), which can be as small as a single word or as large as an entire report. Collectively, themes identified in these units will answer a study’s research question(s). Lincoln and Guba (1985) suggest that a unit of analysis ought to meet two criteria. First, the UOA should be heuristic, that is, it should reveal information pertinent to the study and move the reader to think beyond the singular piece of information. Second, the unit should be the smallest piece of information about something that can stand by itself.” Moreover, a UOA must be interpretable in the absence of any additional information, requiring only that the reader have a broad understanding of the study context.\nFor qualitative investigations of students’ computing code, we propose researchers consider the Block Model as an analytical lens. The Block Model is an educational framework that supports the analysis of the different aspects of a computer program."
  },
  {
    "objectID": "analysis-framework.html#block-model",
    "href": "analysis-framework.html#block-model",
    "title": "Step 1: Selecting a Unit of Analysis",
    "section": "Block Model",
    "text": "Block Model\nIn the Block Model (Schulte 2008), each row represents one selection for the unit of analysis and each column speaks to a different lens of analysis. To decide between the 12 possible options a researcher must consider the context of inquiry. This context dictates the scale of the code that deserves attention.\n\nSchulte, C. 2008. “Block Model: An Educational Model of Program Comprehension as a Tool for a Scholarly Approach to Teaching.” In Proceedings of the Fourth International Workshop on Computing Education Research, 149–60. Sydney, Australia: Association of Computing Machinery; Association of Computing Machinery.\nAn investigation focusing on the broader purpose or structure of a program requires a researcher to zoom out and consider a program’s macrostructure. Whereas, studying individual pieces or segments of a program requires a researcher to zoom in on the atoms or the blocks.\n\n\n\n\n\n\n\n\n\nLevel\n\nDimension\n\n\n\n\n\n\nText Surface\nProgram Execution\nFunction / Purpose\n\n\nMacrostructure\nUnderstanding the overall structure of the program text.\nUnderstanding the algorithm underlying a program.\nUnderstanding the goal/purpose of the program (in the context at hand).\n\n\nRelationships\nRelations & references between blocks (e.g. method calls, object creation, accessing data…)\nSequence of method calls, object sequence diagrams.\nUnderstanding how subgoals are related to goals, how function is achieved by subfunctions.\n\n\nBlocks\nRegions of interest (ROI) that syntactically or semantically build a unit\nOperations of a block, a method, or a ROI (chunk from a set of statements).\nUnderstanding the function of a block, seen as a subgoal.\n\n\nAtoms\nLanguage elements\nOperation of a statement.\nFunction of a statement: its purpose can only be understood in a context."
  },
  {
    "objectID": "analysis-framework.html#analytical-framework-used",
    "href": "analysis-framework.html#analytical-framework-used",
    "title": "Step 1: Selecting a Unit of Analysis",
    "section": "Analytical Framework Used",
    "text": "Analytical Framework Used\nFor the study I will walk you through, I chose to use atoms as my unit of study. An atom constitutes a language element in a program, and can thus have a variety of grain sizes, from characters to words to statements. I chose to have my “atom” be a syntactic statement of R code. A syntactic statement is a line, or set of lines, which constitute the smallest syntactically valid statement of code. Some statements of code were completed in a single line, while others took multiple lines.**As R belongs to the family of “curly bracket” programming languages, influenced by C, R has a syntax which defines statements using curly brackets ({}), many of these statements are introduced by identifiers such as if(), for(), or while().\nFor this study, I selected the program execution as my dimension of analysis. This dimension focuses on the action of each statement, or more specifically what operation it carries out. I chose this dimension as I was interested in understanding what data science skills students were employing within their projects. A program execution lens views these skills as distinct rather than considering their role in the broader function of the program."
  },
  {
    "objectID": "descriptive-codes.html",
    "href": "descriptive-codes.html",
    "title": "Step 2: Descriptive Codes",
    "section": "",
    "text": "The second step in the qualitative analysis is creating qualitative codes for each unit of analysis (UOA).\nThe process of coding in qualitative research, where a researcher makes notes next to each UOA that are potentially relevant to addressing the research question. Codes act as labels, assigning “symbolic meaning” to each UOA (Miles, Huberman, and Saldaña 2020). In our context, each line of R code is a UOA, which is why we are “coding code.”\nThe initial qualitative codes assigned to the units of analysis can be thought of as the “first cycle codes.” There are over 25 different methods for creating first cycle codes, each with a particular focus and purpose. In our paper, we discuss two specific methods of coding we believe are most relevant to investigating computing code: descriptive coding and in vivo coding.\nFor this research, I chose to use descriptive codes as I sought to describe the computing skills students were using in their research project. With in vivo coding, these descriptions would be required to take on the voice (code) of each student, which I felt constrained the possible descriptions available to me."
  },
  {
    "objectID": "descriptive-codes.html#student-a-descriptive-codes",
    "href": "descriptive-codes.html#student-a-descriptive-codes",
    "title": "Step 2: Descriptive Codes",
    "section": "Student A – Descriptive Codes",
    "text": "Student A – Descriptive Codes\nIf you are interested in seeing the original R script file from Student A, you can find that here. If while reading you wonder what the output of a specific statement is, I have reproduced Student A’s code using the penguins dataset (Horst, Hill, and Gorman 2020), available here.\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://allisonhorst.github.io/palmerpenguins/."
  },
  {
    "objectID": "descriptive-codes.html#student-b-descriptive-codes",
    "href": "descriptive-codes.html#student-b-descriptive-codes",
    "title": "Step 2: Descriptive Codes",
    "section": "Student B – Descriptive Codes",
    "text": "Student B – Descriptive Codes\nIf you are interested in seeing the original R script file from Student A, you can find that here."
  },
  {
    "objectID": "themes.html",
    "href": "themes.html",
    "title": "Step 3: Discovering Emergent Themes",
    "section": "",
    "text": "Categories should span multiple codes that were previously identified. These categories “capture some recurring pattern that cuts across your data” (Merriam and Tisdell 2016, 207). Merriam and Tisdell (2016) suggest this process of discovering themes from codes feels somewhat like constantly transitioning one’s perspective of a forest, from looking at the “trees” (codes) to the “forest” (themes) and back to the trees."
  },
  {
    "objectID": "themes.html#discoving-themes",
    "href": "themes.html#discoving-themes",
    "title": "Step 3: Discovering Emergent Themes",
    "section": "Discoving Themes",
    "text": "Discoving Themes\nAs I looked over my descriptive codes, I asked myself what these codes tell me about the nature of the data science skills students used in their projects. Some themes immediately jumped out at me, whereas others took a bit of time to mull over. I’ll walk you through my process below.\n\n“Obvious” Themes\nThere were two themes I expected to see due to the nature of the project and the requirements stipulated by the professor. For their project, students were expected to (1) use an analysis strategy learned in the course and (2) create a visualization to accompany any analysis and resulting discussion. Thus, I expected themes of “Data Model” and “Data Visualization” to emerge from the data.\nFrom my own experiences, I also expected that students would need to perform some aspect of data wrangling to prepare their data for analysis. The data students used for their project were from their own research, so, although I knew data wrangling would play some role, I was unsure what type of tasks might appear in the codes.\n\n\nEmergent Themes\nWhile I was looking over the data wrangling tasks students performed in their projects, I noticed the techniques called upon specific attributes of different data structures (e.g., dataframe, vector, matrix). The implementation of some tasks was fairly uniform (select variable from dataframe using $ operator), whereas other tasks were highly variable. Data filtering was sometimes done with the subset() function, which requires little explicit knowledge of data structures. However, other times this filtering was carried out using the [] / extraction operator, a technique which requires an understanding of how extraction differs across different data structures.\nI also noticed while looking at the R code for the “Data Model” and “Data Visualization” themes that certain statements of code included some knowledge (or lack thereof) regarding the R Environment. The most obvious statement that evoked this theme used with() to temporarily attach a dataframe. There were, however, other statements that also fit into this theme, such as function arguments being bypassed, sourcing in an external R script, loading in datasets, and loading in packages.\nWithin the themes of “Data Model” and “Data Wrangling,” I uncovered an additional theme which speaks to the efficiency of a statement of code. The notion of efficiency came to me from the “don’t repeat yourself” principle (Wilson et al. 2014), which recommends scientists modularize their code rather than copying and pasting and re-use their code instead of rewriting it (p. 2). Thus, I considered code which adhered to these practices “efficient” and code which did not adhere to these practices “inefficient.”\n\nWilson, Greg, D. A. Aruliah, C. Titus Brown, Neil P. Chue Hong, Matt Davis, Richard T. Guy, Steven H. D. Haddock, et al. 2014. “Best Practices for Scientific Computing.” PLOS Biology 12 (1): e1001745.\nThe final theme I discovered were statements of code whose purpose was more for a student’s workflow than anything else. Code comments were my first indication of this theme, where students used code comments to create sections of code or flag what was happening in a particular line / lines of code. I expanded this theme to include statements of code which inspect some characteristic of an object (e.g., structure of a datafame, names of a dataframe, summary of a linear model)."
  },
  {
    "objectID": "themes.html#assigning-descriptive-codes-to-themes",
    "href": "themes.html#assigning-descriptive-codes-to-themes",
    "title": "Step 3: Discovering Emergent Themes",
    "section": "Assigning Descriptive Codes to Themes",
    "text": "Assigning Descriptive Codes to Themes\nFor each of the themes outlined above, the associated “atoms” / statements of code are listed. Keep in mind one statement can apply to two themes! For example, the code\nlinearAnterior <- lm(PADataNoOutlier$Lipid ~ PADataNoOutlier$PSUA)\napplies to three themes. First and foremost, this code uses lm() to fit a linear regression model to the data (data model). Second, in order to fit the data model, the student uses data wrangling to select the variables of interest(PADataNoOutlier$Lipid, PADataNoOutlier$PSUA). Finally, this code does not make use of the data = argument built in to lm(), which implies a lack of understanding of the function and thus the R environment.\n\nData Model\n\n\n\nDefinition: Statements of code whose purpose is to create a statistical model from data.\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n\nDefinition: Statements of code whose purpose is to visualize relationships between variables\nSub-themes\n\nscatterplot\nadding lines to plot\ndifferentiated colors\nincluding a legend\nchanging plotting environment\nmodifying axis labels / plot titles\n\n\n\n\n\n\n\n\n\n\nData Wrangling\n\n\n\nDefinition: Statements of code whose purpose is to prepare a dataset for analysis and / or visualization\nSub-themes\n\nselecting variables\nfiltering observations\nmutating variables\n\n\n\n\n\n\n\n\n\n\nData Structures\n\n\n\nDefinition: An statement of code which explicitly calls upon attributes of a data structure (e.g., dataframe, matrix, vector)\n\n\n\n\n\n\n\n\n\nR Environment\n\n\n\nDefinition: A statement of code which calls on explicit aspects of the R environment\n\n\n\n\n\n\n\n\n\nEfficiency / Inefficiency\n\n\n\nDefinition: A statement of code which adheres to the “don’t repeat yourself” principle\n\n\n\n\n\n\n\n\n\nWorkflow\n\n\n\nDefinition: A statement of code which facilitates a smooth execution of a working process"
  },
  {
    "objectID": "digging-deeper.html",
    "href": "digging-deeper.html",
    "title": "Step 4: Digging Deeper",
    "section": "",
    "text": "I purposefully excluded the words “knowledge” or “understanding” from the definitions of the themes that emerged from the data, because I cannot state that a student “understands” or “knows” a concept solely based on their R code. Additional information is absolutely necessary in order to make this type of statement.\nTherefore, to better understand a student’s conceptual understand of the code they produced, I conducted interviews with each student. I asked both students to articulate for me the purpose of each line of code in their R script, following up with additional clarifying questions if necessary.\nIn each section below I detail how these interviews informed my understanding of the themes outlined previously. Each section begins with a description of each student, providing the reader with an overview of each student’s previous computing experiences prior to the graduate-level Applied Statistics (GLAS) I course."
  },
  {
    "objectID": "digging-deeper.html#student-a",
    "href": "digging-deeper.html#student-a",
    "title": "Step 4: Digging Deeper",
    "section": "Student A",
    "text": "Student A\n\n\n\nThe spring of 2018 was Student A’s first semester as a graduate student, pursuing a master’s degree in Fisheries and Wildlife Management. Student A identified as a Hispanic woman. She had completed a Bachelors in Ecology at a medium sized research university in the western United States. Student A had minimal programming experiences during her bachelor’s degree, with experiences stemming through three main outlets. Helping a postdoc in her lab analyze data from a lab trip exposed her to R, completing the Calculus series for engineers exposed her to MatLab, and enrolling in an information technology course exposed her to Python and Java. In her first semester as a graduate student, Student A enrolled in GLAS, at the recommendation of her adviser.\n\nWorkflow\nSomething I noticed initially when reading through Student A’s R script was the absence of any code importing the dataset(s) used throughout their analysis. Thus, I asked Student A about their process of importing their data into R:\nAllison: “How do you read your data into R?” Student A: “I do import dataset.” Allison: “Ah, you use the”Import Dataset” button in the Enviornment tab to load your dataset in?” Student A: “Mhmm.”\nThrough this conversation, I realized that Student A did not understand how to write code to import their dataset into R. Moreover, they had not recognized the code associated with their “Import Dataset” process appeared in the console once they pressed the “Import” button.\n\n\nReproducibility\nDirectly connected to the absence of code to importing the datasets, the name of a dataset indicated “outliers” had been removed (PADataNoOutlier), yet there were no statements of code performing this data filtering process. Thus, I asked Student A how they had carried out this process:\nAllison: “Right here, it appears that you have removed an outlier from the data, but I don’t see any code related to this removal. How did you do that?”\nStudent A: “In Excel. I know there’s subset()…that I could have subset it. But I forgot how to do that and I was trying to crunch this out, so I just wanted to get the data out, so I just went into Excel and deleted that and then imported the data.\nThis was an eye opening exchange! First and foremost, I discovered that although Student A had used the subset() function in their code, they were not comfortable enough with the function to use it when removing an outlier from a dataset. This implies that, athough data filtering played a major role in Student A’s code, they did not have an understanding of how the subset() function could be applied in a scenario where more than one variable needed to be considered for inclusion / exclusion criteria.\n\n\nR Environment\nThe theme of R Environment manifested in Student A’s code in two ways, (1) the use of with() to temporarily attach a dataset when plotting, and (2) the absence of the data = argument when using lm(). As both processes require the same step–selecting a column from a dataframe–I was intrigued why Student A exclusively used with() when creating data visualizations and the $ operator when using lm():\nAllison: “So, I notice that you are using with() here—with your data, plot these variables. But then here when you are fitting your model, you say fit the model of data$something. I’m wondering why you used with() when making plots and $ when fitting your model.”\nStudent A: “Yeah, and so that [pointing to $ code] is what I think we learned in class. I think we did talk about this, “Oh if you don’t want to do $ and call the data every time, you can do it like that.”\nAllison: “Okay.”\nStudent A: “Um, and then I started doing that [points to with() code], because I found that online and I was like”okay.” And it wasn’t getting an error, I don’t know why I changed, but it wasn’t getting an error.”\nAllison: “I see.”\nStudent A: “And then it worked, so then I just copied and pasted everything and kept working with that.”\nAnother interesting discovery! Student A was simply copying the behavior they had learned in their GLAS course when using the $ within the lm() function rather than utilizing the data = argument. The use of with(), however, Student A did not learn in class, but was gleaned from the internet. Their litmus test for using the code was whether it returned an error, implying Student A did not, in fact, understand the difference between these two methods for extracting variables from a dataframe.\n\n\nExternal Resources\nThis theme of Student A pulling solutions from external resources was found throughout their interview. Asking clarifying details about Student A’s use of transform(), I discovered they were unaware of the general purpose of the function* and the existance of alternative methods for changing the datatype of a variable in a dataframe.***transform() converts its first argument to a data frame.**For example, RPMA2$Age <- as.integer(RPMA2Growth$Age) would have coverted the Age variable without the need to make a new dataframe.\nIt was clear from Student A’s code they had used another student as a resource (#Tanner's code/help), however, it was unclear what portion of their code had been influenced by Tanner. Conversing with Student A, I learned they had discovered a bruteforce method for calculating the mean of specific values from one variable through Googling (Weight1 <- mean(RPMA2GrowthSub$Weight[RPMA2GrowthSub$Age == 1], na.rm = TRUE)). However, after talking with another graduate student about the process they were attempting to carry out, this student (Tanner) offered to send Student A their code. Enter ddply(). Using this code, Student A was able to obtain the mean length / weight across a variety of ages, accomplishing in two lines what had previously taken them 18 lines.******Yet, these 18 lines continued to live in Student A’s code even after she found a more efficient method."
  },
  {
    "objectID": "comparing-students.html",
    "href": "comparing-students.html",
    "title": "Optional: Comparing Across Students",
    "section": "",
    "text": "One of the most substantial differences between Student A’s code and Student B’s was found in the theme of workflow. In Student A’s code, there was no obvious structure to their workflow. Sporadic code comments were used to describe what the code below was for, yet it was also unclear what some comments corresponded to (e.g., #Tanner's code/help). Student A, on the other hand, has a nearly meticulous workflow, starting with sourcing in common functions, then loading in the data, then cleaning the data, and finally analyzing the data. Moreover, Student A used code comments to generate “sections” of code, describing the overall context of the code (e.g., #### Carboy D ####), and “subsections” of code, describing the process being taken (e.g., # Estimate Initial concentration of N15-NO3 relative to Ar).\nInterestingly, there were almost no instances of code in Student B’s analysis where an object was inspected. Alternatively, in Student A’s code there were frequent instances where an object was inspected (e.g., summary(linearAnterior), WeightChange).\n\n\n\n\n\n\n\n\n\n\n\n\n\nAside from student’s use of code comments for organizing their workflow, I noticed differences in their use of whitespace, returns for long lines of code, and named arguments. Whereas Student A would consistently use whitespace surrounding arithmetic operators (e.g., +, -, =. *, ), relational operators (e.g., ==, <, >) operators, and commas, Student B’s use of whitespace was again sporadic. Most frequently, Student A’s statements would have some combination of present and absent whitespace (e.g., Early <- subset(RPMA2GrowthSub, StockYear<2004)).\nSimilar to the use of whitespace, differences were found in how each student handled long lines of code. In all but a few instances of Student B’s code, she used returns to break lines longer than 80 characters. Student A, however, never used returns to break long lines of code. When paired with a lack of whitespace, these long lines made Student A’s code difficult to interpret (as demonstrated in the code below).\n\nwith(PADataNoOutlier, plot(Lipid~log(PSUA), las = 1, col = ifelse(PADataNoOutlier$`Fork Length`<260,\"red\",\"black\")))\n\nInterestingly, Student B would habitually use named arguments for functions she employed. Paired with her use of whitespace and returns, these named arguments made her code more easily readable and digestible. Aside from the code used to produce visualizations (e.g., col =, las =), Student A’s code, however, did not contain references to named arguments. Combined with a sporadic use of whitespace and returns, this lack of named arguments made Student A’s code difficult to read and interpret the processes being enacted.\nBelow are two examples of code which contrast all three of these instances of “readability”:\nStudent A\n\nEarlyLengthAge <- ddply(Early, ~Age, summarise, meanLE=mean(ForkLength, na.rm = T))\n\nStudent B\n\nlikelihoods <- apply(X = pMat,\n                     MARGIN = 1, \n                     FUN = nmle, \n                     t = timeD, \n                     y = obsD,  \n                     N15_NO3_O = fracDenD*(N15_NO3_O_D)\n)\n\n\n\n\n\n\n\nAs mentioned previously, at the beginning of Student B’s code were explicit references to the data being used for analysis. Specifically, Student B used the load() function to source her data. Rather than writing statements of code, Student A instead used the RStudio GUI to import her data into her workspace. Thus, in Student A’s code there are no lines of code which load in the data she worked with. Not only does this make Student A’s code not reproducible, but references to dataframes named PADataNoOutlier become increasingly concerning. When asked about how the “outliers” were removed from the PADataNoOutlier dataset, Student A stated that she had used Excel to clean the data and then loaded the cleaned data into RStudio (using the GUI).\nStudent A’s code had additional statements which raise concerns for reproducibilty. Specifically, there are statements which call on the ddply() function before the plyr package has been loaded. In addition, Student A had two instances of script fragments, code which would not execute or which would not produce the desired result (displayed below). The first instance (plot(LengthAge$mean ~ LengthAge$Age)) references a non-existent variable (LengthAge$mean). The second instance attempts to create a dataframe of previously created objects, but the Growth column is not correctly created, as Student A neglects to use the c() function to combine these objects into a vector.\n\n\n\n\n\n\n\n\nA Note About Student’s Script Files\nBoth Student A and Student B interacted with R through R scripts created in RStudio. While R Markdown (Allaire et al. 2022) documents existed at the time of their GLAS course, the instructor of the course did not demonstrate the use of these dynamic documents. Thus, these student’s analysis copied and pasted their results from RStudio into a Word file.\n\nAllaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2022. Rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown.\nAlthough it was noted that Student B used functions (e.g., source(), load()) to load functions and data into her R script, these statements used a mix of full and relative paths to access these materials. This mix of full and relative paths also makes Student B’s script limited in its reproducibility. It is, however, worth noting that at the time of their GLAS course, RStudio projects did not exist. Thus, the methods\n\n\n\nViewing these differences through the lens of these student’s computing experiences, helps us understand potential reasons for why these differences occurred. As discussed in Digging Deeper, Student A entered graduate school (and GLAS) with hardly any computing experiences. Student B, however, entered graduate school having numerous experiences programming in Matlab, and completed the Swirl tutorial (Kross et al. 2018) before enrolling in GLAS.\n\nKross, Sean, Nick Carchedi, Bill Bauer, Gina Grdina, Filip Schouwenaars, and Wush Wu. 2018. Swirl: Learn r, in r. https://CRAN.R-project.org/package=swirl.\nStudent B’s prior programming experiences provided her with an appreciation for structured programs, as well as an understanding of the importance of reproducible code. Due to her limited programming experiences, Student A’s attention was pulled toward getting a working solution rather that writing readable code, organizing her code, or ensuring her analysis was fully reproducible."
  },
  {
    "objectID": "comparing-students.html#efficiency-inefficiency",
    "href": "comparing-students.html#efficiency-inefficiency",
    "title": "Optional: Comparing Across Students",
    "section": "Efficiency / Inefficiency",
    "text": "Efficiency / Inefficiency\n\n\n\n\nWilson, Greg, D. A. Aruliah, C. Titus Brown, Neil P. Chue Hong, Matt Davis, Richard T. Guy, Steven H. D. Haddock, et al. 2014. “Best Practices for Scientific Computing.” PLOS Biology 12 (1): e1001745.\nEfficiency of student’s code was determined based on a statement’s adheres to the “don’t repeat yourself” principle (Wilson et al. 2014). Student B’s prior programming experiences allowed her to see the importance of writing efficient code, sourcing in functions she frequently used and utilizing iteration for repeated operations (e.g., apply()). With her limited programming experiences, Student A was unfamiliar with this programming practice. Instead, Student A was focused on finding a working solution for the task at hand. Thus, when a working solution was found, Student A would copy, paste, and modify the code to suit a variety of situations."
  },
  {
    "objectID": "comparing-students.html#data-visualization",
    "href": "comparing-students.html#data-visualization",
    "title": "Optional: Comparing Across Students",
    "section": "Data Visualization",
    "text": "Data Visualization\nDespite the considerable differences in Student A and Student B’s workflow and programming efficiency, they had substantial similarities in the data visualizations they produced. Both students primarily produced scatterplots, often including a third variable by coloring points. Both students would consistently modify their axis labels, rotate their axis tick mark labels (las), and include a legend in their plot. Each of these similarities arose from their experiences in the GLAS course, where these practices were modeled by the instructor for the visualizations the class produced.\nThere are, however, notable differences within these similarities. Where Student A paired the plot() and lines() functions, Student B used the built-in type argument to produce a line plot. Additionally, Student B’s scatterplot had more polished axis labels through her use of the title() function. Finally, although small in nature, each student used a different method to declare the legend position, with Student A specifying x and y coordinates and Student B using the (“bottomright”) string specification."
  },
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "Allison Theobold",
    "section": "",
    "text": "Allison Theobold is an Associate Professor of Statistics at California Polytechnic University in beautiful San Luis Obispo, California. Allison’s work focuses on innovation in statistics and data science education, with an emphasis on equitable pedagogy and learning trajectories. Allison is also interested in exploring pedagogical approaches for enhancing retention of under-represented students in STEM, including creating inclusive discoursive spaces and equitable group collaborations."
  },
  {
    "objectID": "bio.html#education",
    "href": "bio.html#education",
    "title": "Allison Theobold",
    "section": "Education",
    "text": "Education\nPhD in Statistics, Montana State University — Bozeman, MT\nM.S. in Statistics, Montana State University — Bozeman, MT\nB.S. in Mathematics, Colorado Mesa University — Grand Junction, CO\nB.B.A. in Economics, Colorado Mesa University — Grand Junction, CO"
  },
  {
    "objectID": "bio.html#publications",
    "href": "bio.html#publications",
    "title": "Allison Theobold",
    "section": "Publications",
    "text": "Publications\nTheobold, A. S. & Williams. “I watched as he put things on the paper”: A Feminist View of Mathematical Discourse, Psychology of Mathematics Education North America (PME-NA) Conference.\nAmal Abdel-Ghani, Kelly Bodwin, Amelia McNamara, Allison Theobold, and Ian Flores Siaca (2022). “Looks okay to me”: A study of best practice in data analysis code review, International Conference on Teaching Statistics (ICOTS) Conference.\nTheobold, A. (2021). Oral Exams: A More Meaningful Assessment of Statistical Understanding}, Journal of Statistics and Data Science Education, Brief Commentary.\nTheobold, A. S. & Williams, D. A. (2021). Discourse Patterns in a Small Group “Collaboration”: The Case of Uma and Sean. In Karunakaran, S. S. & Higgins, A. (Eds.), 2021 Research in Undergraduate Mathematics Education Reports (pp. 324-331).\nTheobold, A., Hancock, S., & Mannheimer, S. (2021). Designing Data Science Workshops for Data-Intensive Environmental Science Research, Journal of Statistics Education, 29(sup1), S83-S94.\nTheobold, A. and Hancock, S. (2019). How Environmental Science Graduate Students Acquire Statistical Computing Skills, Statistics Education Research Journal, 18(2), 68-85."
  },
  {
    "objectID": "data/studentA_code.html",
    "href": "data/studentA_code.html",
    "title": "Student A Code",
    "section": "",
    "text": "#upper anterior measurement Linear model\n        linearAnterior <- lm(PADataNoOutlier$Lipid~PADataNoOutlier$PSUA)\n        summary(linearAnterior)\n        linearAnterior\n        with(PADataNoOutlier, plot(Lipid~PSUA,las = 1,col = ifelse(PADataNoOutlier$`Fork Length`< 280,\"red\",\"black\")))\n        abline(linearAnterior)\n        plot(linearAnterior)\n\n        #Exponential function\n        expAnterior <- lm(PADataNoOutlier$Lipid~log(PADataNoOutlier$PSUA))\n        summary (expAnterior)\n        expAnterior\n        with(PADataNoOutlier, plot(Lipid~log(PSUA), las = 1, col = ifelse(PADataNoOutlier$`Fork Length`< 260,\"red\",\"black\")))\n        abline(expAnterior)\n        plot(expAnterior)\n        summary(expAnterior)\n\n        early <- subset(RPMA2Growth, StockYear<2006)\n        mid <- subset(RPMA2Growth, StockYear<2014 & StockYear>2003)\n        RPMA2GrowthSub <- transform(RPMA2Growth, Age = as.integer(Age))\n        Early <- subset(RPMA2GrowthSub, StockYear<2004)\n        Mid <- subset(RPMA2GrowthSub, StockYear<2018 & StockYear>2005)\n        EarlyWeightAge <- ddply(Early, ~Age, summarise, meanWE=mean(Weight, na.rm = T))\n        EarlyLengthAge <- ddply(Early, ~Age, summarise, meanLE=mean(ForkLength, na.rm = T))\n        MidLengthAge <- ddply(Mid, ~Age, summarise, meanLM=mean(ForkLength, na.rm = T))\n        WeightChange <- rep(NA, 9)\n\n        library(plyr)\n        WeightAge <- ddply(RPMA2GrowthSub, ~Age, summarise, meanW=mean(Weight, na.rm = T))\n        LengthAge <- ddply(RPMA2GrowthSub, ~Age, summarise, meanL=mean(ForkLength, na.rm = T))\n        plot(EarlyLengthAge$meanLE~EarlyLengthAge$Age,las = 1,ylab = \"Fork Length (mm)\",xlab = \"Age\")\n        lines(EarlyLengthAge$meanLE~EarlyLengthAge$Age)\n        points(MidLengthAge$meanLM~MidLengthAge$Age,col = \"red\")\n        lines(MidLengthAge$meanLM~MidLengthAge$Age,col = \"red\")\n        legend(15, 600, legend = c(\"1998-2003\", \"2006-2017\"),col = c(\"black\", \"red\"), lty = 1:1,cex = 0.8)\n\n        #Tanner's code/help\n        WeightChange <- rep(NA, 9)\n        library(plyr)\n        WeightAge <- ddply(RPMA2GrowthSub, ~Age, summarise, meanW=mean(Weight, na.rm = T))\n        LengthAge <- ddply(RPMA2GrowthSub, ~Age, summarise, meanL=mean(ForkLength, na.rm = T))\n        plot(WeightAge$meanW~WeightAge$Age)\n        plot(LengthAge$mean~LengthAge$Age)\n        WeightChange\n\n        Weight1 <- mean(RPMA2GrowthSub$Weight[RPMA2GrowthSub$Age==1], na.rm=TRUE)\n        Weight1\n        Length1 <- mean(RPMA2GrowthSub$ForkLength[RPMA2GrowthSub$Age==1], na.rm=TRUE)\n        Weight2 <- mean(RPMA2GrowthSub$Weight[RPMA2GrowthSub$Age==2], na.rm=TRUE)\n        Length2 <- mean(RPMA2GrowthSub$ForkLength[RPMA2GrowthSub$Age==2], na.rm=TRUE)\n        Weight3 <- mean(RPMA2GrowthSub$Weight[RPMA2GrowthSub$Age==3], na.rm=TRUE)\n        Length3 <- mean(RPMA2GrowthSub$ForkLength[RPMA2GrowthSub$Age==3], na.rm=TRUE)\n        Weight4 <- mean(RPMA2GrowthSub$Weight[RPMA2GrowthSub$Age==4], na.rm=TRUE)\n        Length4 <- mean(RPMA2GrowthSub$ForkLength[RPMA2GrowthSub$Age==4], na.rm=TRUE)\n        Weight5 <- mean(RPMA2GrowthSub$Weight[RPMA2GrowthSub$Age==5], na.rm=TRUE)\n        Length5 <- mean(RPMA2GrowthSub$ForkLength[RPMA2GrowthSub$Age==5], na.rm=TRUE)\n        Weight6 <- mean(RPMA2GrowthSub$Weight[RPMA2GrowthSub$Age==6], na.rm=TRUE)\n        Length6 <- mean(RPMA2GrowthSub$ForkLength[RPMA2GrowthSub$Age==6], na.rm=TRUE)\n        Weight7 <- mean(RPMA2GrowthSub$Weight[RPMA2GrowthSub$Age==7], na.rm=TRUE)\n        Length7 <- mean(RPMA2GrowthSub$ForkLength[RPMA2GrowthSub$Age==7], na.rm=TRUE)\n        Weight8 <- mean(RPMA2GrowthSub$Weight[RPMA2GrowthSub$Age==8], na.rm=TRUE)\n        Length8 <- mean(RPMA2GrowthSub$ForkLength[RPMA2GrowthSub$Age==8], na.rm=TRUE)\n        Weight9 <- mean(RPMA2GrowthSub$Weight[RPMA2GrowthSub$Age==9], na.rm=TRUE)\n        Length9 <- mean(RPMA2GrowthSub$ForkLength[RPMA2GrowthSub$Age==9], na.rm=TRUE)\n        x <- data.frame(\"Age\" = 1:9, \"Growth\" = Weight1,Weight2,Weight3,Weight4,Weight5,Weight6,Weight7,Weight8,Weight9)"
  },
  {
    "objectID": "data/studentB_code.html",
    "href": "data/studentB_code.html",
    "title": "Student B Code",
    "section": "",
    "text": "rm(list = ls())\n        source(\"./Gas_Functions.R\")\n\n        # Load data ####\n        load(\"***REDACTED***/gas\")\n        load(\"***REDACTED***/carboys\")\n\n        gas <- gas[!(substr(gas$sampleID, 3, 3) %in% c(\"b\", \"c\")), ]\n        gas$days <- as.numeric(gas$minutesSinceAmendment / (24 * 60))\n\n        # Calculate molar fraction of N15-N2\n        RstN <- 0.003678\n        R <- ((gas$delN2 / 1000) + 1) * RstN\n        gas$N15_MF <- R / (1 + R)\n\n\n        # Calculate concentration of N15-N14 N2 relative to Argon\n        gas$N15_N2_Ar <- (gas$N15_MF * gas$N2Ar) * (40 / 28.014) \n\n        #mol N15-N2 per mol Ar\n        # Function to calculate likelihood of parameters given data ####\n\n        nmle <- function(P, t, y, N15_NO3_O){\n          yhat <- N15_NO3_O * (1 - exp(-P[1] * t))\n          -sum(dnorm(y, yhat, exp(P[2]), log = T))\n        }\n\n\n        #### Carboy D ####\n\n        # Make vectors for time and N15-N2 observations\n\n        timeD <- (subset(gas, gas$carboy == \"D\"))$days\n        obsD <- subset(gas, gas$carboy == \"D\")$N15_N2_Ar\n        timeD <- timeD[!is.na(obsD)]\n        obsD <- obsD[!is.na(obsD)]\n\n        # Subtract off N15-N2 initially present in sample and set tracer N15-N2 to 0 at t=0\n        obsD <- obsD - obsD[1]\n\n        # Estimate Initial concentration of N15-NO3 relative to Ar\n\n        N15_NO3_O_D <- 40 * (\n                              (carboys[carboys$CarboyID == \"D\",]$EstN15NO3) + \n                              (0.7 * RstN / (1 + RstN))\n                            ) / (subset(gas, gas$carboy == \"D\")$Ar[1])\n\n        # Estimate fraction of labeled nitrate that gets denitrified \n\n        fracDenD = max(obsD) / N15_NO3_O_D\n\n        # Search for best parameters\n\n        mle.outD <- nlm(f = nmle, p = c(1, 0.01), t = timeD, y = obsD, \n                        N15_NO3_O = N15_NO3_O_D*fracDenD)\n        ktotEst <- mle.outD$estimate[1] \n        kuEst <- ktotEst * (1 - fracDenD )\n        kDenEst <- ktotEst * fracDenD \n\n        #per day\n\n        sigmaEst <- exp(mle.outD$estimate[2])\n\n        # Plot model with data\n\n        quartz(width = 4.5, height = 4)\n        par(mar = c(3.5, 4, 3, 1))\n\n        predictionTimesD <- seq(0, max(timeD), length.out = 100)\n        predictionD <- fracDenD * N15_NO3_O_D * (1 - exp(-ktotEst * predictionTimesD))\n\n        plot(x = predictionTimesD,\n             y = predictionD, \n             col = \"blue\", type = \"l\", \n             xlab = \"\" , \n             ylab = \"\",\n             ylim = c(0,0.08),\n             main = \"Mesocosm D\",\n             las = 1)\n        points(timeD, obsD, pch = 19)\n        title(ylab = expression(paste(\"Tracer \"^15, N[2],\":Ar\")), \n              line = 2.5, font.sub = 2)\n        title(xlab = \"Time (days)\", line = 2,font.sub =2)\n        legend(\"bottomright\", legend = c(\"Modeled\", \"Measured\"), \n               lty = c(\"solid\", NA), col = c(\"blue\", \"black\"), \n               pch = c(NA, 19))\n\n        # Calculate confidence interval\n\n        # Make matrix of parameter combinations\n\n        numRows <- 1000\n        kTotMin <- 2\n        kTotMax <- 60\n\n        pMat <- matrix(\n                      data = c(seq(kTotMin, kTotMax, length.out = numRows), \n                      rep(log(sigmaEst), times = numRows)),\n                      nrow = numRows)\n\n        likelihoods <- apply(X = pMat,\n                             MARGIN = 1, \n                             FUN = nmle, \n                             t = timeD, \n                             y = obsD,  \n                             N15_NO3_O = fracDenD*(N15_NO3_O_D)\n                             )\n\n        mlle <- -min(likelihoods)\n        mlleIndex <- which.min(likelihoods)\n        mlleCI <- mlle - 1.96\n\n        lowerCIBound <- pMat[1:mlleIndex,1][which.min(abs(mlleCI + likelihoods[1:mlleIndex]))]\n        upperCIBound <- pMat[mlleIndex:length(likelihoods),1][which.min(abs(mlleCI + likelihoods[mlleIndex:length(likelihoods)]))]\n\n        CI <- c(lowerCIBound, upperCIBound)\n        print(CI)\n\n        lowerCIBoundkDen <- lowerCIBound * fracDenD\n        upperCIBoundkDen <- upperCIBound * fracDenD\n\n\n        # Plot likelihoods with confidence intervals\n\n        quartz(width = 4.5, height = 4)\n        plot(x = seq(kTotMin,kTotMax, length.out = numRows),\n             y = -likelihoods,\n             type = \"l\",\n             xlab = \"ktot (per day)\",\n             ylab = \"log(Likelihood)\",\n             las = 1)\n\n        abline(v = lowerCIBound, lty = 2, col = \"blue\")\n        abline(v = upperCIBound, lty = 2, col = \"blue\")\n\n\n        #### Carboy E ####\n\n        # Make vectors for time and N15-N2 observations\n\n        timeE <- (subset(gas, gas$carboy == \"E\"))$days\n        obsE <- subset(gas, gas$carboy == \"E\")$N15_N2_Ar\n        timeE <- timeE[!is.na(obsE)]\n        obsE <- obsE[!is.na(obsE)]\n\n\n        # Subtract off N15-N2 initially present in sample and set tracer N15-N2 to 0 at t=0\n\n        obsE <- obsE - obsE[1]\n\n        # Estimate Initial concentration of N15-NO3 relative to Ar\n\n        N15_NO3_O_E <- 40*((carboys[carboys$CarboyID == \"E\",]$EstN15NO3) + (0.7*RstN/(1+RstN)))/(subset(gas, gas$carboy == \"E\")$Ar[1])\n\n        # Estimate fraction of labeled nitrate that gets denitrified \n\n        fracDenE = max(obsE) / N15_NO3_O_E\n\n        # Search for best parameters\n\n        mle.outE <- nlm(f = nmle, p = c(1, 0.01), t = timeE, y = obsE, \n                        N15_NO3_O = N15_NO3_O_E * fracDenE)\n\n        ktotEst <- mle.outE$estimate[1] \n\n        #per day\n\n        kuEst <- ktotEst * (1 - fracDenE )\n        kDenEst <- ktotEst * fracDenE \n\n        #per day\n\n        sigmaEst <- exp(mle.outE$estimate[2])\n\n        # Plot model with data\n\n        quartz(width = 4.5, height = 4)\n        par(mar = c(3.5, 4, 3, 1))\n\n        predictionTimesE <- seq(0,max(timeE), length.out = 100)\n        predictionE <- fracDenE * N15_NO3_O_E * (1 - exp(-ktotEst * predictionTimesE))\n        plot(x = predictionTimesE, \n             y = predictionE, \n             col = \"blue\", type = \"l\", \n             xlab = \"\" , \n             ylab = \"\",\n             ylim = c(0,0.08),\n             main = \"Mesocosm E\",\n             las = 1)\n        points(timeE, obsE, pch = 19)\n        title(ylab = expression(paste(\"Tracer \"^15, N[2],\":Ar\")), \n              line = 2.5, font.sub = 2)\n        title(xlab = \"Time (days)\", line = 2,font.sub =2)\n        legend(\"bottomright\", legend = c(\"Modeled\", \"Measured\"), \n               lty = c(\"solid\", NA), col = c(\"blue\", \"black\"), \n               pch = c(NA, 19))\n\n        # Calculate confidence interval\n        # Make matrix of parameter combinations\n\n        numRows <- 1000\n        kTotMin <- 2\n        kTotMax <- 10\n\n        pMat <- matrix(data = c(seq(kTotMin, kTotMax, length.out = numRows), \n                                rep(log(sigmaEst), times = numRows)),\n                       nrow = numRows)\n\n        likelihoods <- apply(X = pMat, \n                             MARGIN = 1, \n                             FUN = nmle, \n                             t = timeE, \n                             y = obsE, \n                             N15_NO3_O = fracDenE*(N15_NO3_O_E)\n                             )\n\n        mlle <- -min(likelihoods)\n        mlleIndex <- which.min(likelihoods)\n        mlleCI <- mlle - 1.96\n\n        lowerCIBound <- pMat[1:mlleIndex,1][which.min(abs(mlleCI + likelihoods[1:mlleIndex]))]\n\n        upperCIBound <- pMat[mlleIndex:length(likelihoods),1][which.min(abs(mlleCI + likelihoods[mlleIndex:length(likelihoods)]))]\n\n        CI <- c(lowerCIBound, upperCIBound)\n        print(CI)\n\n        lowerCIBoundkDen <- lowerCIBound * fracDenE\n        upperCIBoundkDen <- upperCIBound * fracDenE\n\n        # Plot likelihoods with confidence intervals\n\n        quartz(width = 4.5, height = 4)\n\n        plot(x = seq(kTotMin,kTotMax, \n             length.out = numRows),\n             y = -likelihoods,\n             type = \"l\",\n             xlab = \"ktot (per day)\",\n             ylab = \"log(Likelihood)\",\n             las = 1)\n        abline(v = lowerCIBound, lty = 2, col = \"blue\")\n        abline(v = upperCIBound, lty = 2, col = \"blue\")\n\n\n        #### Carboy F ####\n\n        # Make vectors for time and N15-N2 observations\n\n        timeF <- (subset(gas, gas$carboy == \"F\"))$days\n        obsF <- subset(gas, gas$carboy == \"F\")$N15_N2_Ar\n        timeF <- timeF[!is.na(obsF)]\n        obsF <- obsF[!is.na(obsF)]\n\n        # Subtract off N15-N2 initially present in sample and set tracer N15-N2 to 0 at t=0\n\n        obsF <- obsF - obsF[1]\n\n        # Estimate Initial concentration of N15-NO3 relative to Ar\n\n        N15_NO3_O_F <- 40 * (\n                              (carboys[carboys$CarboyID == \"F\",]$EstN15NO3) + \n                              (0.7 * RstN / (1 + RstN))\n                              ) / (subset(gas, gas$carboy == \"F\")$Ar[1])\n\n        # Estimate fraction of labeled nitrate that gets denitrified \n\n        fracDenF = max(obsF) / N15_NO3_O_F\n\n        # Search for best parameters\n\n        mle.outF <- nlm(f = nmle, p = c(1, 0.01), t = timeF, y = obsF, \n                        N15_NO3_O = N15_NO3_O_F * fracDenF)\n        ktotEst <- mle.outF$estimate[1] \n\n        #per day\n\n        kuEst <- ktotEst * (1 - fracDenF )\n        kDenEst <- ktotEst * fracDenF \n\n        #per day\n\n        sigmaEst <- exp(mle.outE$estimate[2])\n\n        # Plot model with data\n\n        quartz(width = 4.5, height = 4)\n        par(mar = c(3.5, 4, 3, 1))\n\n        predictionTimesF <- seq(0, max(timeF), length.out = 100)\n        predictionF <- fracDenF * N15_NO3_O_F * (1 - exp(-ktotEst * predictionTimesF))\n\n        plot(x = predictionTimesF, \n             y = predictionF, \n             col = \"blue\", type = \"l\", \n             xlab = \"\" , \n             ylab = \"\",\n             ylim = c(0,0.08),\n             main = \"Mesocosm F\",\n             las = 1)\n        points(timeF, obsF, pch = 19)\n        title(ylab = expression(paste(\"Tracer \"^15, N[2],\":Ar\")), \n              line = 2.5, font.sub = 2)\n        title(xlab = \"Time (days)\", line = 2,font.sub =2)\n        legend(\"bottomright\", legend = c(\"Modeled\", \"Measured\"), \n               lty = c(\"solid\", NA), col = c(\"blue\", \"black\"), \n               pch = c(NA, 19))\n\n        # Calculate confidence interval\n\n        # Make matrix of parameter combinations\n\n        numRows <- 1000\n        kTotMin <- 2\n        kTotMax <- 5\n\n        pMat <- matrix(data = c(seq(kTotMin, kTotMax, length.out = numRows), \n                                rep(log(sigmaEst), times = numRows)),\n                       nrow = numRows)\n\n        likelihoods <- apply(X = pMat, \n                             MARGIN = 1, \n                             FUN = nmle, \n                             t = timeF, \n                             y = obsF, \n                             N15_NO3_O = fracDenF*(N15_NO3_O_F)\n                             )\n\n        mlle <- -min(likelihoods)\n        mlleIndex <- which.min(likelihoods)\n        mlleCI <- mlle - 1.96\n\n        lowerCIBound <- pMat[1:mlleIndex,1][which.min(abs(mlleCI + likelihoods[1:mlleIndex]))]\n        upperCIBound <- pMat[mlleIndex:length(likelihoods),1][which.min(abs(mlleCI + likelihoods[mlleIndex:length(likelihoods)]))]\n\n        CI <- c(lowerCIBound, upperCIBound)\n        print(CI)\n\n        lowerCIBoundkDen <- lowerCIBound * fracDenF\n        upperCIBoundkDen <- upperCIBound * fracDenF\n\n        # Plot likelihoods with confidence intervals\n\n        quartz(width = 4.5, height = 4)\n\n        plot(x = seq(kTotMin,kTotMax, \n             length.out = numRows),\n             y = -likelihoods,\n             type = \"l\",\n             xlab = \"ktot (per day)\",\n             ylab = \"log(Likelihood)\",\n             las = 1)\n        abline(v = lowerCIBound, lty = 2, col = \"blue\")\n        abline(v = upperCIBound, lty = 2, col = \"blue\")"
  },
  {
    "objectID": "data/studentA_reproduced.html",
    "href": "data/studentA_reproduced.html",
    "title": "Reproducing Student A’s Code",
    "section": "",
    "text": "The following analysis reproduces Student A’s code in the context of the penguins dataset from the palmerpenguins package (Horst, Hill, and Gorman 2020).\nThe only variable type missing from the penguins dataset is a variable similar to Student A’s “Age” variable. The pengins dataset contains information on adult penguins. Depending on the size of the penguin, they can take between three and eight years until they begin breeding. Most species of penguins take at least five years. Thus, when creating an age variable in the penguins dataset I simulated values from a Normal distribution with mean 5 and standard deviation 1."
  },
  {
    "objectID": "data/studentA_reproduced.html#reproducing-student-as-code",
    "href": "data/studentA_reproduced.html#reproducing-student-as-code",
    "title": "Reproducing Student A’s Code",
    "section": "Reproducing Student A’s Code",
    "text": "Reproducing Student A’s Code\n\n#upper anterior measurement Linear model\nlinearAnterior <- lm(penguins$bill_length_mm~penguins$bill_depth_mm)\nsummary(linearAnterior)\nlinearAnterior\nwith(penguins, plot(bill_length_mm~bill_depth_mm,las = 1,col = ifelse(penguins$body_mass_g< 4050,\"red\",\"black\")))\nabline(linearAnterior)\nplot(linearAnterior)\n\n\n#Exponential function\nexpAnterior <- lm(penguins$bill_length_mm~log(penguins$bill_depth_mm))\nsummary (expAnterior)\nexpAnterior\nwith(penguins, plot(bill_length_mm~log(bill_depth_mm), las = 1, col = ifelse(penguins$body_mass_g< 4050,\"red\",\"black\")))\nabline(expAnterior)\nplot(expAnterior)\nsummary(expAnterior)\n\n\nearly <- subset(penguins, year<2007)\nmid <- subset(penguins, year<2009 & year>2007)\npenguinsSub <- transform(penguins, Age = as.integer(age))\nEarly <- subset(penguinsSub, year<2008)\nMid <- subset(penguinsSub, year<2009 & year>2007)\nEarlyWeightAge <- ddply(Early, ~Age, summarise, meanWE=mean(body_mass_g, na.rm = T))\nEarlyLengthAge <- ddply(Early, ~Age, summarise, meanLE=mean(bill_length_mm, na.rm = T))\nMidLengthAge <- ddply(Mid, ~Age, summarise, meanLM=mean(bill_length_mm, na.rm = T))\nWeightChange <- rep(NA, 9)\n\nlibrary(plyr)\nWeightAge <- ddply(penguinsSub, ~Age, summarise, meanW=mean(body_mass_g, na.rm = T))\nLengthAge <- ddply(penguinsSub, ~Age, summarise, meanL=mean(bill_length_mm, na.rm = T))\nplot(EarlyLengthAge$meanLE~EarlyLengthAge$Age,las = 1,ylab = \"Fork Length (mm)\",xlab = \"Age\")\nlines(EarlyLengthAge$meanLE~EarlyLengthAge$Age)\npoints(MidLengthAge$meanLM~MidLengthAge$Age,col = \"red\")\nlines(MidLengthAge$meanLM~MidLengthAge$Age,col = \"red\")\nlegend(6, 48, legend = c(\"1998-2003\", \"2006-2017\"),col = c(\"black\", \"red\"), lty = 1:1,cex = 0.8)\n\n#Tanner's code/help\nWeightChange <- rep(NA, 9)\nlibrary(plyr)\nWeightAge <- ddply(penguinsSub, ~Age, summarise, meanW=mean(body_mass_g, na.rm = T))\nLengthAge <- ddply(penguinsSub, ~Age, summarise, meanL=mean(bill_length_mm, na.rm = T))\nplot(WeightAge$meanW~WeightAge$Age)\nplot(LengthAge$mean~LengthAge$Age)\nWeightChange\n\nWeight1 <- mean(penguinsSub$body_mass_g[penguinsSub$Age==2], na.rm=TRUE)\nWeight1\nLength1 <- mean(penguinsSub$bill_length_mm[penguinsSub$Age==2], na.rm=TRUE)\nWeight2 <- mean(penguinsSub$body_mass_g[penguinsSub$Age==3], na.rm=TRUE)\nLength2 <- mean(penguinsSub$bill_length_mm[penguinsSub$Age==3], na.rm=TRUE)\nWeight3 <- mean(penguinsSub$body_mass_g[penguinsSub$Age==4], na.rm=TRUE)\nLength3 <- mean(penguinsSub$bill_length_mm[penguinsSub$Age==4], na.rm=TRUE)\nWeight4 <- mean(penguinsSub$body_mass_g[penguinsSub$Age==5], na.rm=TRUE)\nLength4 <- mean(penguinsSub$bill_length_mm[penguinsSub$Age==5], na.rm=TRUE)\nWeight5 <- mean(penguinsSub$body_mass_g[penguinsSub$Age==6], na.rm=TRUE)\nLength5 <- mean(penguinsSub$bill_length_mm[penguinsSub$Age==6], na.rm=TRUE)\nWeight6 <- mean(penguinsSub$body_mass_g[penguinsSub$Age==7], na.rm=TRUE)\nLength6 <- mean(penguinsSub$bill_length_mm[penguinsSub$Age==7], na.rm=TRUE)\n# Weight7 - Length9 removed\nx <- data.frame(\"Age\" = 1:7, \"Growth\" = Weight1,Weight2,Weight3,Weight4,Weight5,Weight6)"
  }
]
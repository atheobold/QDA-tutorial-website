[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Coding Code: Qualitative Methods for Investigating Data Science Skills",
    "section": "",
    "text": "Students are creative thinkers, and their code provides a window into their learning process. A qualitative analysis of a student’s code can provide insight into their creative process and collective learning process, rather than focusing solely on whether the code is executable. Moreover, qualitative methods allow for the comparison across students’ code to identify learning trajectories that may exist and potential growth points."
  },
  {
    "objectID": "blog.html#structure-of-tutorial",
    "href": "blog.html#structure-of-tutorial",
    "title": "A Guide to Qualitatively Analyzing Computing Code",
    "section": "Structure of Tutorial",
    "text": "Structure of Tutorial\nThe pages of this website correspond to different aspects of the analysis process as pictured above.\n\nI discuss how one selects a unit of analysis\nI outline how descriptive codes are created\nI walk through how codes can be collapsed into emergent themes\nI describe how student interviews allowed me to dig deeper into the themes that emerged\nI talk through how to make comparisons across individual’s themes"
  },
  {
    "objectID": "analysis-framework.html",
    "href": "analysis-framework.html",
    "title": "Step 1: Selecting a Unit of Analysis",
    "section": "",
    "text": "The process of data analysis begins by identifying segments in your data set that are responsive to your research questions” (Merriam and Tisdell 2016). These segments form the units of analysis (UOA), which can be as small as a single word or as large as an entire report. Collectively, themes identified in these units will answer a study’s research question(s). Lincoln and Guba (1985) suggest that a unit of analysis ought to meet two criteria. First, the UOA should be heuristic, that is, it should reveal information pertinent to the study and move the reader to think beyond the singular piece of information. Second, the unit should be the smallest piece of information about something that can stand by itself.” Moreover, a UOA must be interpretable in the absence of any additional information, requiring only that the reader have a broad understanding of the study context.\nFor qualitative investigations of students’ computing code, we propose researchers consider the Block Model as an analytical lens. The Block Model is an educational framework that supports the analysis of the different aspects of a computer program."
  },
  {
    "objectID": "analysis-framework.html#block-model",
    "href": "analysis-framework.html#block-model",
    "title": "Step 1: Selecting a Unit of Analysis",
    "section": "Block Model",
    "text": "Block Model\nIn the Block Model (Schulte 2008), each row represents one selection for the unit of analysis and each column speaks to a different lens of analysis. To decide between the 12 possible options a researcher must consider the context of inquiry. This context dictates the scale of the code that deserves attention.\n\nSchulte, C. 2008. “Block Model: An Educational Model of Program Comprehension as a Tool for a Scholarly Approach to Teaching.” In Proceedings of the Fourth International Workshop on Computing Education Research, 149–60. Sydney, Australia: Association of Computing Machinery; Association of Computing Machinery.\nAn investigation focusing on the broader purpose or structure of a program requires a researcher to zoom out and consider a program’s macrostructure. Whereas, studying individual pieces or segments of a program requires a researcher to zoom in on the atoms or the blocks.\n\n\n\n\n\n\n\n\n\nLevel\n\nDimension\n\n\n\n\n\n\nText Surface\nProgram Execution\nFunction / Purpose\n\n\nMacrostructure\nUnderstanding the overall structure of the program text.\nUnderstanding the algorithm underlying a program.\nUnderstanding the goal/purpose of the program (in the context at hand).\n\n\nRelationships\nRelations & references between blocks (e.g. method calls, object creation, accessing data…)\nSequence of method calls, object sequence diagrams.\nUnderstanding how subgoals are related to goals, how function is achieved by subfunctions.\n\n\nBlocks\nRegions of interest (ROI) that syntactically or semantically build a unit\nOperations of a block, a method, or a ROI (chunk from a set of statements).\nUnderstanding the function of a block, seen as a subgoal.\n\n\nAtoms\nLanguage elements\nOperation of a statement.\nFunction of a statement: its purpose can only be understood in a context."
  },
  {
    "objectID": "analysis-framework.html#analytical-framework-used",
    "href": "analysis-framework.html#analytical-framework-used",
    "title": "Step 1: Selecting a Unit of Analysis",
    "section": "Analytical Framework Used",
    "text": "Analytical Framework Used\nFor the study I will walk you through, I chose to use atoms as my unit of study. An atom constitutes a language element in a program, and can thus have a variety of grain sizes, from characters to words to statements. I chose to have my “atom” be a syntactic statement of R code. A syntactic statement is a line, or set of lines, which constitute the smallest syntactically valid statement of code. Some statements of code were completed in a single line, while others took multiple lines.**As R belongs to the family of “curly bracket” programming languages, influenced by C, R has a syntax which defines statements using curly brackets ({}), many of these statements are introduced by identifiers such as if(), for(), or while().\nFor this study, I selected the program execution as my dimension of analysis. This dimension focuses on the action of each statement, or more specifically what operation it carries out. I chose this dimension as I was interested in understanding what data science skills students were employing within their projects. A program execution lens views these skills as distinct rather than considering their role in the broader function of the program."
  },
  {
    "objectID": "descriptive-codes.html",
    "href": "descriptive-codes.html",
    "title": "Step 2: Descriptive Codes",
    "section": "",
    "text": "The second step in the qualitative analysis is creating qualitative codes for each unit of analysis (UOA).\nThe process of coding in qualitative research, where a researcher makes notes next to each UOA that are potentially relevant to addressing the research question. Codes act as labels, assigning “symbolic meaning” to each UOA (Miles, Huberman, and Saldaña 2020). In our context, each line of R code is a UOA, which is why we are “coding code.”\nThe initial qualitative codes assigned to the units of analysis can be thought of as the “first cycle codes.” There are over 25 different methods for creating first cycle codes, each with a particular focus and purpose. In our paper, we discuss two specific methods of coding we believe are most relevant to investigating computing code: descriptive coding and in vivo coding.\nFor this research, I chose to use descriptive codes as I sought to describe the computing skills students were using in their research project. With in vivo coding, these descriptions would be required to take on the voice (code) of each student, which I felt constrained the possible descriptions available to me."
  },
  {
    "objectID": "descriptive-codes.html#student-a-descriptive-codes",
    "href": "descriptive-codes.html#student-a-descriptive-codes",
    "title": "Step 2: Descriptive Codes",
    "section": "Student A – Descriptive Codes",
    "text": "Student A – Descriptive Codes\nIf you are interested in seeing the original R script file from Student A, you can find that here."
  },
  {
    "objectID": "descriptive-codes.html#student-b-descriptive-codes",
    "href": "descriptive-codes.html#student-b-descriptive-codes",
    "title": "Step 2: Descriptive Codes",
    "section": "Student B – Descriptive Codes",
    "text": "Student B – Descriptive Codes\nIf you are interested in seeing the original R script file from Student A, you can find that here."
  },
  {
    "objectID": "themes.html",
    "href": "themes.html",
    "title": "Step 3: Discovering Emergent Themes",
    "section": "",
    "text": "Categories should span multiple codes that were previously identified. These categories “capture some recurring pattern that cuts across your data” (Merriam and Tisdell 2016, 207). Merriam and Tisdell (2016) suggest this process of discovering themes from codes feels somewhat like constantly transitioning one’s perspective of a forest, from looking at the “trees” (codes) to the “forest” (themes) and back to the trees."
  },
  {
    "objectID": "themes.html#discoving-themes",
    "href": "themes.html#discoving-themes",
    "title": "Step 3: Discovering Emergent Themes",
    "section": "Discoving Themes",
    "text": "Discoving Themes\nAs I looked over my descriptive codes, I asked myself what these codes tell me about the nature of the data science skills students used in their projects. Some themes immediately jumped out at me, whereas others took a bit of time to mull over. I’ll walk you through my process below.\n\n“Obvious” Themes\nThere were two themes I expected to see due to the nature of the project and the requirements stipulated by the professor. For their project, students were expected to (1) use an analysis strategy learned in the course and (2) create a visualization to accompany any analysis and resulting discussion. Thus, I expected themes of “Data Model” and “Data Visualization” to emerge from the data.\nFrom my own experiences, I also expected that students would need to perform some aspect of data wrangling to prepare their data for analysis. The data students used for their project were from their own research, so, although I knew data wrangling would play some role, I was unsure what type of tasks might appear in the codes.\n\n\nEmergent Themes\nWhile I was looking over the data wrangling tasks students performed in their projects, I noticed the techniques called upon specific attributes of different data structures (e.g., dataframe, vector, matrix). The implementation of some tasks was fairly uniform (select variable from dataframe using $ operator), whereas other tasks were highly variable. Data filtering was sometimes done with the subset() function, which requires little explicit knowledge of data structures. However, other times this filtering was carried out using the [] / extraction operator, a technique which requires an understanding of how extraction differs across different data structures.\nI also noticed while looking at the R code for the “Data Model” and “Data Visualization” themes that certain statements of code included some knowledge (or lack thereof) regarding the R Environment. The most obvious statement that evoked this theme used with() to temporarily attach a dataframe. There were, however, other statements that also fit into this theme, such as function arguments being bypassed, sourcing in an external R script, loading in datasets, and loading in packages.\nWithin the themes of “Data Model” and “Data Wrangling,” I uncovered an additional theme which speaks to the efficiency of a statement of code. The notion of efficiency came to me from the “don’t repeat yourself” principle (Wilson et al. 2014), which recommends scientists modularize their code rather than copying and pasting and re-use their code instead of rewriting it (p. 2). Thus, I considered code which adhered to these practices “efficient” and code which did not adhere to these practices “inefficient.”\n\nWilson, Greg, D. A. Aruliah, C. Titus Brown, Neil P. Chue Hong, Matt Davis, Richard T. Guy, Steven H. D. Haddock, et al. 2014. “Best Practices for Scientific Computing.” PLOS Biology 12 (1): e1001745.\nThe final theme I discovered were statements of code whose purpose was more for a student’s workflow than anything else. Code comments were my first indication of this theme, where students used code comments to create sections of code or flag what was happening in a particular line / lines of code. I expanded this theme to include statements of code which inspect some characteristic of an object (e.g., structure of a datafame, names of a dataframe, summary of a linear model)."
  },
  {
    "objectID": "themes.html#assigning-descriptive-codes-to-themes",
    "href": "themes.html#assigning-descriptive-codes-to-themes",
    "title": "Step 3: Discovering Emergent Themes",
    "section": "Assigning Descriptive Codes to Themes",
    "text": "Assigning Descriptive Codes to Themes\nFor each of the themes outlined above, the associated “atoms” / statements of code are listed. Keep in mind one statement can apply to two themes! For example, the code\nlinearAnterior <- lm(PADataNoOutlier$Lipid ~ PADataNoOutlier$PSUA)\napplies to three themes. First and foremost, this code uses lm() to fit a linear regression model to the data (data model). Second, in order to fit the data model, the student uses data wrangling to select the variables of interest(PADataNoOutlier$Lipid, PADataNoOutlier$PSUA). Finally, this code does not make use of the data = argument built in to lm(), which implies a lack of understanding of the function and thus the R environment.\n\nData Model\n\n\n\nDefinition: Statements of code whose purpose is to create a statistical model from data.\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n\nDefinition: Statements of code whose purpose is to visualize relationships between variables\nSub-themes\n\nscatterplot\nadding lines to plot\ndifferentiated colors\nincluding a legend\nchanging plotting environment\nmodifying axis labels / plot titles\n\n\n\n\n\n\n\n\n\n\nData Wrangling\n\n\n\nDefinition: Statements of code whose purpose is to prepare a dataset for analysis and / or visualization\nSub-themes\n\nselecting variables\nfiltering observations\nmutating variables\n\n\n\n\n\n\n\n\n\n\nData Structures\n\n\n\nDefinition: An statement of code which explicitly calls upon attributes of a data structure (e.g., dataframe, matrix, vector)\n\n\n\n\n\n\n\n\n\nR Environment\n\n\n\nDefinition: A statement of code which calls on explicit aspects of the R environment\n\n\n\n\n\n\n\n\n\nEfficiency / Inefficiency\n\n\n\nDefinition: A statement of code which adheres to the “don’t repeat yourself” principle\n\n\n\n\n\n\n\n\n\nWorkflow\n\n\n\nDefinition: A statement of code which facilitates a smooth execution of a working process"
  },
  {
    "objectID": "digging-deeper.html",
    "href": "digging-deeper.html",
    "title": "Step 4: Digging Deeper",
    "section": "",
    "text": "I purposefully excluded the words “knowledge” or “understanding” from the definitions of the themes that emerged from the data, because I cannot state that a student “understands” or “knows” a concept solely based on their R code. Additional information is absolutely necessary in order to make this type of statement.\nTherefore, to better understand a student’s conceptual understand of the code they produced, I conducted interviews with each student. I asked both students to articulate for me the purpose of each line of code in their R script, following up with additional clarifying questions if necessary.\nIn each section below I detail how these interviews informed my understanding of the themes outlined previously. Each section begins with a description of each student, providing the reader with an overview of each student’s previous computing experiences prior to the graduate-level Applied Statistics (GLAS) I course."
  },
  {
    "objectID": "digging-deeper.html#student-a",
    "href": "digging-deeper.html#student-a",
    "title": "Step 4: Digging Deeper",
    "section": "Student A",
    "text": "Student A\n\n\n\nThe spring of 2018 was Student A’s first semester as a graduate student, pursuing a master’s degree in Fisheries and Wildlife Management. Student A identified as a Hispanic woman. She had completed a Bachelors in Ecology at a medium sized research university in the western United States. Student A had minimal programming experiences during her bachelor’s degree, with experiences stemming through three main outlets. Helping a postdoc in her lab analyze data from a lab trip exposed her to R, completing the Calculus series for engineers exposed her to MatLab, and enrolling in an information technology course exposed her to Python and Java. In her first semester as a graduate student, Student A enrolled in GLAS, at the recommendation of her adviser.\n\nWorkflow\nSomething I noticed initially when reading through Student A’s R script was the absence of any code importing the dataset(s) used throughout their analysis. Thus, I asked Student A about their process of importing their data into R:\nAllison: “How do you read your data into R?” Student A: “I do import dataset.” Allison: “Ah, you use the”Import Dataset” button in the Enviornment tab to load your dataset in?” Student A: “Mhmm.”\nThrough this conversation, I realized that Student A did not understand how to write code to import their dataset into R. Moreover, they had not recognized the code associated with their “Import Dataset” process appeared in the console once they pressed the “Import” button.\n\n\nReproducibility\nDirectly connected to the absence of code to importing the datasets, the name of a dataset indicated “outliers” had been removed (PADataNoOutlier), yet there were no statements of code performing this data filtering process. Thus, I asked Student A how they had carried out this process:\nAllison: “Right here, it appears that you have removed an outlier from the data, but I don’t see any code related to this removal. How did you do that?”\nStudent A: “In Excel. I know there’s subset()…that I could have subset it. But I forgot how to do that and I was trying to crunch this out, so I just wanted to get the data out, so I just went into Excel and deleted that and then imported the data.\nThis was an eye opening exchange! First and foremost, I discovered that although Student A had used the subset() function in their code, they were not comfortable enough with the function to use it when removing an outlier from a dataset. This implies that, athough data filtering played a major role in Student A’s code, they did not have an understanding of how the subset() function could be applied in a scenario where more than one variable needed to be considered for inclusion / exclusion criteria.\n\n\nR Environment\nThe theme of R Environment manifested in Student A’s code in two ways, (1) the use of with() to temporarily attach a dataset when plotting, and (2) the absence of the data = argument when using lm(). As both processes require the same step–selecting a column from a dataframe–I was intrigued why Student A exclusively used with() when creating data visualizations and the $ operator when using lm():\nAllison: “So, I notice that you are using with() here—with your data, plot these variables. But then here when you are fitting your model, you say fit the model of data$something. I’m wondering why you used with() when making plots and $ when fitting your model.”\nStudent A: “Yeah, and so that [pointing to $ code] is what I think we learned in class. I think we did talk about this, “Oh if you don’t want to do $ and call the data every time, you can do it like that.”\nAllison: “Okay.”\nStudent A: “Um, and then I started doing that [points to with() code], because I found that online and I was like”okay.” And it wasn’t getting an error, I don’t know why I changed, but it wasn’t getting an error.”\nAllison: “I see.”\nStudent A: “And then it worked, so then I just copied and pasted everything and kept working with that.”\nAnother interesting discovery! Student A was simply copying the behavior they had learned in their GLAS course when using the $ within the lm() function rather than utilizing the data = argument. The use of with(), however, Student A did not learn in class, but was gleaned from the internet. Their litmus test for using the code was whether it returned an error, implying Student A did not, in fact, understand the difference between these two methods for extracting variables from a dataframe.\n\n\nExternal Resources\nThis theme of Student A pulling solutions from external resources was found throughout their interview. Asking clarifying details about Student A’s use of transform(), I discovered they were unaware of the general purpose of the function* and the existance of alternative methods for changing the datatype of a variable in a dataframe.***transform() converts its first argument to a data frame.**For example, RPMA2$Age <- as.integer(RPMA2Growth$Age) would have coverted the Age variable without the need to make a new dataframe.\nIt was clear from Student A’s code they had used another student as a resource (#Tanner's code/help), however, it was unclear what portion of their code had been influenced by Tanner. Conversing with Student A, I learned they had discovered a bruteforce method for calculating the mean of specific values from one variable through Googling (Weight1 <- mean(RPMA2GrowthSub$Weight[RPMA2GrowthSub$Age == 1], na.rm = TRUE)). However, after talking with another graduate student about the process they were attempting to carry out, this student (Tanner) offered to send Student A their code. Enter ddply(). Using this code, Student A was able to obtain the mean length / weight across a variety of ages, accomplishing in two lines what had previously taken them 18 lines.******Yet, these 18 lines continued to live in Student A’s code even after she found a more efficient method."
  },
  {
    "objectID": "comparing-students.html",
    "href": "comparing-students.html",
    "title": "Optional: Comparing Across Students",
    "section": "",
    "text": "Efficiency / Inefficiency\n\n\n\n\n\n\n\n\n\n\n\n\n\nReadability\n\n\n\n\n\n\nReproducibility\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\nA: So, the code that you found on the internet, did it have this format? Like, with the data, plot y ~ x? And then this piece, but this piece is different. It’s not, like not, most people don’t use that piece.\nM: Oh, that’s a Kezia piece."
  },
  {
    "objectID": "bio.html",
    "href": "bio.html",
    "title": "Allison Theobold",
    "section": "",
    "text": "Allison Theobold is an Associate Professor of Statistics at California Polytechnic University in beautiful San Luis Obispo, California. Allison’s work focuses on innovation in statistics and data science education, with an emphasis on equitable pedagogy and learning trajectories. Allison is also interested in exploring pedagogical approaches for enhancing retention of under-represented students in STEM, including creating inclusive discoursive spaces and equitable group collaborations."
  },
  {
    "objectID": "bio.html#education",
    "href": "bio.html#education",
    "title": "Allison Theobold",
    "section": "Education",
    "text": "Education\nPhD in Statistics, Montana State University — Bozeman, MT\nM.S. in Statistics, Montana State University — Bozeman, MT\nB.S. in Mathematics, Colorado Mesa University — Grand Junction, CO\nB.B.A. in Economics, Colorado Mesa University — Grand Junction, CO"
  },
  {
    "objectID": "bio.html#publications",
    "href": "bio.html#publications",
    "title": "Allison Theobold",
    "section": "Publications",
    "text": "Publications\nTheobold, A. S. & Williams. “I watched as he put things on the paper”: A Feminist View of Mathematical Discourse, Psychology of Mathematics Education North America (PME-NA) Conference.\nAmal Abdel-Ghani, Kelly Bodwin, Amelia McNamara, Allison Theobold, and Ian Flores Siaca (2022). “Looks okay to me”: A study of best practice in data analysis code review, International Conference on Teaching Statistics (ICOTS) Conference.\nTheobold, A. (2021). Oral Exams: A More Meaningful Assessment of Statistical Understanding}, Journal of Statistics and Data Science Education, Brief Commentary.\nTheobold, A. S. & Williams, D. A. (2021). Discourse Patterns in a Small Group “Collaboration”: The Case of Uma and Sean. In Karunakaran, S. S. & Higgins, A. (Eds.), 2021 Research in Undergraduate Mathematics Education Reports (pp. 324-331).\nTheobold, A.}, Hancock, S., & Mannheimer, S. (2021). Designing Data Science Workshops for Data-Intensive Environmental Science Research}, Journal of Statistics Education, 29(sup1), S83-S94.\nTheobold, A. and Hancock, S. (2019). How Environmental Science Graduate Students Acquire Statistical Computing Skills}, Statistics Education Research Journal, 18(2), 68-85."
  }
]